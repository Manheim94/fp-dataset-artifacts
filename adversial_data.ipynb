{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Generate training and validation Adversarial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install textattack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import textattack\n",
    "\n",
    "def apply_attack_to_premise(dataset, attack):\n",
    "    transformed_premise_data = []\n",
    "    for original_premise, original_hypothesis, label in dataset:\n",
    "        attack_results = attack.attack(original_premise, label)\n",
    "\n",
    "        if hasattr(attack_results, '__iter__'):\n",
    "            for result in attack_results:\n",
    "                if isinstance(result, textattack.attack_results.SuccessfulAttackResult):\n",
    "                    transformed_premise_data.append((original_premise, result.perturbed_text(), original_hypothesis, label))\n",
    "        else:\n",
    "            if isinstance(attack_results, textattack.attack_results.SuccessfulAttackResult):\n",
    "                transformed_premise_data.append((original_premise, attack_results.perturbed_text(), original_hypothesis, label))\n",
    "    return transformed_premise_data\n",
    "\n",
    "def apply_attack_to_hypothesis(dataset, attack):\n",
    "    transformed_hypothesis_data = []\n",
    "    for original_premise, original_hypothesis, label in dataset:\n",
    "        attack_results = attack.attack(original_hypothesis, label)\n",
    "\n",
    "        if hasattr(attack_results, '__iter__'):\n",
    "            for result in attack_results:\n",
    "                if isinstance(result, textattack.attack_results.SuccessfulAttackResult):\n",
    "                    transformed_hypothesis_data.append((original_premise, original_hypothesis, result.perturbed_text(), label))\n",
    "        else:\n",
    "            if isinstance(attack_results, textattack.attack_results.SuccessfulAttackResult):\n",
    "                transformed_hypothesis_data.append((original_premise, original_hypothesis, attack_results.perturbed_text(), label))\n",
    "    return transformed_hypothesis_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
    "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
    "from textattack.constraints.grammaticality import PartOfSpeech\n",
    "from textattack.transformations import WordSwapEmbedding\n",
    "from textattack.search_methods import GreedySearch\n",
    "from textattack.goal_functions import UntargetedClassification\n",
    "from textattack import Attack\n",
    "\n",
    "# customized receipe to our specific problem\n",
    "class CustomRecipe(Attack):\n",
    "    def __init__(self, model):\n",
    "        transformation = WordSwapEmbedding(max_candidates=10)\n",
    "        constraints = [\n",
    "            RepeatModification(),\n",
    "            StopwordModification(),\n",
    "            UniversalSentenceEncoder(threshold=0.8),\n",
    "            PartOfSpeech()\n",
    "        ]\n",
    "\n",
    "        search_method = GreedySearch()\n",
    "        goal_function = UntargetedClassification(model)\n",
    "        super().__init__(goal_function, constraints, transformation, search_method)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
    "from textattack.datasets import Dataset\n",
    "from transformers import ElectraForSequenceClassification\n",
    "\n",
    "model_path = '/content/NLP_FP/nlp_fp/trained_model/checkpoint/checkpoint-206000'\n",
    "model = ElectraForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Wrap the model for TextAttack\n",
    "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
    "\n",
    "# Initialize your TextAttack custom recipe with the model wrapper\n",
    "attack = CustomRecipe(model_wrapper)\n",
    "\n",
    "class CustomTextAttackDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import subprocess\n",
    "import torch\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "num_examples = 1000\n",
    "total_examples = 10000\n",
    "\n",
    "\n",
    "for i in range(0, total_examples, num_examples):\n",
    "\n",
    "    limited_combined_data = [(row['premise'], row['hypothesis'], row['label']) \n",
    "                             for _, row in df_ad.iloc[i:i+num_examples].iterrows()]\n",
    "\n",
    "    custom_dataset = CustomTextAttackDataset(limited_combined_data)\n",
    "    transformed_premise_data = apply_attack_to_premise(custom_dataset, attack)\n",
    "    structured_premise_data = []\n",
    "\n",
    "    for orig_premise, adv_premise, orig_hypothesis, label in transformed_premise_data:\n",
    " \n",
    "        structured_premise_data.append({\n",
    "            'premise': adv_premise,\n",
    "            'hypothesis': orig_hypothesis,\n",
    "            'label': label\n",
    "        })\n",
    "  \n",
    "    file_name = f'transformed_premise_data_{i // num_examples + 1}.json'\n",
    "\n",
    "    with open(file_name, 'w') as outfile:\n",
    "        json.dump(structured_premise_data, outfile, indent=4)\n",
    "\n",
    "    subprocess.run([\"git\", \"add\", file_name])\n",
    "    commit_message = f\"Add processed chunk {i // num_examples + 1}\"\n",
    "    subprocess.run([\"git\", \"commit\", \"-m\", commit_message])\n",
    "    print(f\"Processed, saved, and committed chunk {i // num_examples + 1}\")\n",
    "\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.email\", \"joliefang@utexas.edu\"])\n",
    "subprocess.run([\"git\", \"config\", \"--global\", \"user.name\", \"JoFangUTA\"])\n",
    "subprocess.run([\"git\", \"push\", \"origin\", \"main\"])\n",
    "\n",
    "print(\"All chunks processed, saved, committed, and pushed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import logging\n",
    "\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "num_files = 50\n",
    "directory = \"/content/NLP_FP/nlp_fp/\"\n",
    "file_names = [f'{directory}transformed_hypothesis_data_{i}.json' for i in range(1, num_files + 1)]\n",
    "combined_data = []\n",
    "missing_files = []\n",
    "\n",
    "for file_name in file_names:\n",
    "    if os.path.exists(file_name):\n",
    "        try:\n",
    "            with open(file_name, 'r') as file:\n",
    "                data = json.load(file)\n",
    "                combined_data.extend(data)\n",
    "        except json.JSONDecodeError:\n",
    "            logging.error(f\"Error decoding JSON from {file_name}\")\n",
    "    else:\n",
    "        missing_files.append(file_name)\n",
    "\n",
    "output_file = f'{directory}new_combined_transformed_hypothesis_data.json'\n",
    "try:\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        json.dump(combined_data, outfile, indent=4)\n",
    "    logging.info(f\"All files have been combined into '{output_file}'\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error writing to combined file: {e}\")\n",
    "\n",
    "if missing_files:\n",
    "    logging.warning(f\"The following files were not found: {', '.join(missing_files)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records: 16482\n",
      "Training records: 13185\n",
      "Validation records: 3297\n"
     ]
    }
   ],
   "source": [
    "# split dataset into training and validation\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_path = '/Users/joliefang/Downloads/adver_Data/cleaned_addata/cleaned_final_data.json'\n",
    "with open(data_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_data, validation_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Save the training data\n",
    "with open('/Users/joliefang/Downloads/adver_Data/cleaned_addata/train_data.json', 'w') as file:\n",
    "    json.dump(train_data, file)\n",
    "\n",
    "# Save the validation data\n",
    "with open('/Users/joliefang/Downloads/adver_Data/cleaned_addata/validation_data.json', 'w') as file:\n",
    "    json.dump(validation_data, file)\n",
    "\n",
    "print(f\"Total records: {len(data)}\")\n",
    "print(f\"Training records: {len(train_data)}\")\n",
    "print(f\"Validation records: {len(validation_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03 13:29:44.948741: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 8338.58it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 194.41it/s]\n",
      "Generating train split: 3297 examples [00:00, 101474.35 examples/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100%|████████████| 3297/3297 [00:00<00:00, 6022.77 examples/s]\n",
      "/Users/joliefang/anaconda3/envs/test_env/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|█████████████████████████████████████████| 413/413 [02:09<00:00,  3.20it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 0.8307645916938782, 'eval_accuracy': 0.7992113828659058, 'eval_runtime': 129.7323, 'eval_samples_per_second': 25.414, 'eval_steps_per_second': 3.183}\n"
     ]
    }
   ],
   "source": [
    "# python3 run.py --do_eval --task nli --dataset snli --model ./trained_model/checkpoint-206000   --output_dir ./eval_output/snli/\n",
    "!python3 /Users/joliefang/Downloads/adver_Data/fp_run/run.py --do_eval --task nli --dataset /Users/joliefang/Downloads/adver_Data/adversarial_data/validation_data.json   --model /Users/joliefang/Downloads/checkpoint-206000  --output_dir /Users/joliefang/Downloads/adver_Data/adversarial_data/eval_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"eval_loss\": 0.8307645916938782, \"eval_accuracy\": 0.7992113828659058, \"eval_runtime\": 129.7323, \"eval_samples_per_second\": 25.414, \"eval_steps_per_second\": 3.183}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune on Adversarial Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03 11:16:03.919620: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 5433.04it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 155.13it/s]\n",
      "Generating train split: 13185 examples [00:00, 111314.65 examples/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100%|██████████| 13185/13185 [00:03<00:00, 4343.04 examples/s]\n",
      "/Users/joliefang/anaconda3/envs/test_env/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "  0%|                                                  | 0/4947 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "{'loss': 0.338, 'learning_rate': 4.494643218111988e-05, 'epoch': 0.3}           \n",
      "{'loss': 0.3532, 'learning_rate': 3.9892864362239746e-05, 'epoch': 0.61}        \n",
      "{'loss': 0.2934, 'learning_rate': 3.4839296543359614e-05, 'epoch': 0.91}        \n",
      "{'loss': 0.2245, 'learning_rate': 2.9785728724479482e-05, 'epoch': 1.21}        \n",
      "{'loss': 0.1916, 'learning_rate': 2.4732160905599354e-05, 'epoch': 1.52}        \n",
      "{'loss': 0.2142, 'learning_rate': 1.9678593086719225e-05, 'epoch': 1.82}        \n",
      "{'loss': 0.1895, 'learning_rate': 1.4625025267839095e-05, 'epoch': 2.12}        \n",
      "{'loss': 0.1157, 'learning_rate': 9.571457448958965e-06, 'epoch': 2.43}         \n",
      "{'loss': 0.1285, 'learning_rate': 4.517889630078836e-06, 'epoch': 2.73}         \n",
      "{'train_runtime': 6636.8204, 'train_samples_per_second': 5.96, 'train_steps_per_second': 0.745, 'train_loss': 0.2185205213224042, 'epoch': 3.0}\n",
      "100%|█████████████████████████████████████| 4947/4947 [1:50:36<00:00,  1.34s/it]\n"
     ]
    }
   ],
   "source": [
    "!python3 /Users/joliefang/Downloads/adver_Data/fp_run/run.py --do_train --task nli --dataset ./train_data.json --output_dir /Users/joliefang/Downloads/adver_Data/output  --model /Users/joliefang/Downloads/checkpoint-206000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03 13:36:03.871163: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100%|████████████| 3297/3297 [00:00<00:00, 5451.67 examples/s]\n",
      "/Users/joliefang/anaconda3/envs/test_env/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|█████████████████████████████████████████| 413/413 [02:10<00:00,  3.17it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 0.46119940280914307, 'eval_accuracy': 0.9126478433609009, 'eval_runtime': 131.047, 'eval_samples_per_second': 25.159, 'eval_steps_per_second': 3.152}\n"
     ]
    }
   ],
   "source": [
    "!python3 /Users/joliefang/Downloads/adver_Data/fp_run/run.py --do_eval --task nli --dataset /Users/joliefang/Downloads/adver_Data/adversarial_data/validation_data.json   --model /Users/joliefang/Downloads/adver_Data/output/checkpoint-4500 --output_dir /Users/joliefang/Downloads/adver_Data/finetune_ad_eval_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "{\"eval_loss\": 0.46119940280914307, \"eval_accuracy\": 0.9126478433609009, \"eval_runtime\": 131.047, \"eval_samples_per_second\": 25.159, \"eval_steps_per_second\": 3.152}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03 13:43:00.310875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100%|████████████| 9842/9842 [00:01<00:00, 5151.32 examples/s]\n",
      "/Users/joliefang/anaconda3/envs/test_env/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|███████████████████████████████████████| 1231/1231 [06:16<00:00,  3.27it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 0.8638477921485901, 'eval_accuracy': 0.8518593907356262, 'eval_runtime': 376.954, 'eval_samples_per_second': 26.109, 'eval_steps_per_second': 3.266}\n"
     ]
    }
   ],
   "source": [
    "!python3 /Users/joliefang/Downloads/adver_Data/fp_run/run.py --do_eval --task nli --dataset snli  --model /Users/joliefang/Downloads/adver_Data/output/checkpoint-4500 --output_dir /Users/joliefang/Downloads/adver_Data/finetune_og_snli_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════════════╤════════════════════╤═════════════════════════════╕\n",
      "│ Model             │   accuracy on SNLI │   accuracy on SNLI-contrast │\n",
      "╞═══════════════════╪════════════════════╪═════════════════════════════╡\n",
      "│ Pretrain          │              89.47 │                       45.41 │\n",
      "├───────────────────┼────────────────────┼─────────────────────────────┤\n",
      "│ Contrast-Finetune │              86.4  │                       94    │\n",
      "╘═══════════════════╧════════════════════╧═════════════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "# create data\n",
    "data = [[\"Pretrain\", 89.47, 45.41], \n",
    "        [\"Contrast-Finetune\", 86.40, 94.00]]\n",
    "  \n",
    "#define header names\n",
    "col_names = [\"Model\", \"accuracy on SNLI\", \"accuracy on SNLI-contrast\"]\n",
    "  \n",
    "#display table\n",
    "print(tabulate(data, headers=col_names, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-12-03 21:53:17.039846: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Downloading data files: 100%|███████████████████| 1/1 [00:00<00:00, 7570.95it/s]\n",
      "Extracting data files: 100%|█████████████████████| 1/1 [00:00<00:00, 224.46it/s]\n",
      "Generating train split: 4204 examples [00:00, 150019.60 examples/s]\n",
      "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
      "Map (num_proc=2): 100%|████████████| 4204/4204 [00:01<00:00, 3552.97 examples/s]\n",
      "/Users/joliefang/anaconda3/envs/test_env/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|█████████████████████████████████████████| 526/526 [02:51<00:00,  3.06it/s]\n",
      "Evaluation results:\n",
      "{'eval_loss': 4.061384201049805, 'eval_accuracy': 0.42102760076522827, 'eval_runtime': 172.5979, 'eval_samples_per_second': 24.357, 'eval_steps_per_second': 3.048}\n"
     ]
    }
   ],
   "source": [
    "!python3 /Users/joliefang/Downloads/adver_Data/fp_run/run.py --do_eval --task nli --dataset /Users/joliefang/Downloads/adver_Data/adversarial_data/contrast_validation.jsonl  --model /Users/joliefang/Downloads/adver_Data/output/checkpoint-4500 --output_dir /Users/joliefang/Downloads/adver_Data/contrast_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bcbecc1ab860cb059c568f2b3e72029f1530d06f216b3aa5d7ca0d49f3a3ace2"
  },
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
