{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfN80LPLqeUY",
        "outputId": "2a6eb022-64f8-4f6a-9891-3d735a368a4d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate (from -r requirements.txt (line 1))\n",
            "  Downloading accelerate-0.25.0-py3-none-any.whl (265 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets (from -r requirements.txt (line 2))\n",
            "  Downloading datasets-2.15.0-py3-none-any.whl (521 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m521.2/521.2 kB\u001b[0m \u001b[31m45.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (2.1.0+cu118)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.66.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (4.35.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (6.0.1)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.19.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 1)) (0.4.1)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (9.0.0)\n",
            "Collecting pyarrow-hotfix (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.4.1)\n",
            "Collecting multiprocess (from datasets->-r requirements.txt (line 2))\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->-r requirements.txt (line 2)) (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->-r requirements.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 5)) (0.15.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->-r requirements.txt (line 2)) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets->-r requirements.txt (line 2)) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->-r requirements.txt (line 3)) (2.1.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->-r requirements.txt (line 2)) (2023.3.post1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->-r requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets->-r requirements.txt (line 2)) (1.16.0)\n",
            "Installing collected packages: pyarrow-hotfix, dill, multiprocess, accelerate, datasets\n",
            "Successfully installed accelerate-0.25.0 datasets-2.15.0 dill-0.3.7 multiprocess-0.70.15 pyarrow-hotfix-0.6\n"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_G6ExCPtk6r"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3SQaiQBtKK5"
      },
      "source": [
        "### model training\n",
        "\n",
        "*   task: nli\n",
        "*   model: google/electra-small-discriminator\n",
        "*   dataset: snli\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZTcbwv_jtsm8",
        "outputId": "8bbcbe77-91c2-47d5-a47c-f008677bfef5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-11-19 02:56:01.075180: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-19 02:56:01.075238: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-19 02:56:01.075279: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-19 02:56:02.816974: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading builder script: 100% 3.82k/3.82k [00:00<00:00, 19.0MB/s]\n",
            "Downloading metadata: 100% 1.90k/1.90k [00:00<00:00, 11.9MB/s]\n",
            "Downloading readme: 100% 14.1k/14.1k [00:00<00:00, 46.2MB/s]\n",
            "Downloading: 100% 1.93k/1.93k [00:00<00:00, 8.68MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 9.20MB/s]\n",
            "Downloading: 100% 65.9M/65.9M [00:01<00:00, 58.0MB/s]\n",
            "Downloading: 100% 1.26M/1.26M [00:00<00:00, 9.08MB/s]\n",
            "config.json: 100% 665/665 [00:00<00:00, 4.26MB/s]\n",
            "pytorch_model.bin: 100% 54.2M/54.2M [00:00<00:00, 202MB/s]\n",
            "Some weights of ElectraForSequenceClassification were not initialized from the model checkpoint at google/electra-small-discriminator and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "tokenizer_config.json: 100% 29.0/29.0 [00:00<00:00, 166kB/s]\n",
            "vocab.txt: 100% 232k/232k [00:00<00:00, 7.99MB/s]\n",
            "tokenizer.json: 100% 466k/466k [00:00<00:00, 30.3MB/s]\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 125140.35 examples/s]\n",
            "Filter: 100% 550152/550152 [00:02<00:00, 267919.32 examples/s]\n",
            "Filter: 100% 10000/10000 [00:00<00:00, 271718.69 examples/s]\n",
            "Map (num_proc=2): 100% 549367/549367 [01:37<00:00, 5620.54 examples/s]\n",
            "  0% 0/206013 [00:00<?, ?it/s]You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "{'loss': 0.9363, 'learning_rate': 4.987864843480751e-05, 'epoch': 0.01}\n",
            "{'loss': 0.7443, 'learning_rate': 4.975729686961503e-05, 'epoch': 0.01}\n",
            "{'loss': 0.6967, 'learning_rate': 4.963594530442254e-05, 'epoch': 0.02}\n",
            "{'loss': 0.6506, 'learning_rate': 4.951459373923005e-05, 'epoch': 0.03}\n",
            "{'loss': 0.6327, 'learning_rate': 4.9393242174037564e-05, 'epoch': 0.04}\n",
            "{'loss': 0.6013, 'learning_rate': 4.927189060884507e-05, 'epoch': 0.04}\n",
            "{'loss': 0.6067, 'learning_rate': 4.9150539043652585e-05, 'epoch': 0.05}\n",
            "{'loss': 0.5905, 'learning_rate': 4.90291874784601e-05, 'epoch': 0.06}\n",
            "{'loss': 0.5983, 'learning_rate': 4.890783591326761e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5825, 'learning_rate': 4.8786484348075126e-05, 'epoch': 0.07}\n",
            "{'loss': 0.5731, 'learning_rate': 4.866513278288264e-05, 'epoch': 0.08}\n",
            "{'loss': 0.5611, 'learning_rate': 4.8543781217690146e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5735, 'learning_rate': 4.842242965249766e-05, 'epoch': 0.09}\n",
            "{'loss': 0.5541, 'learning_rate': 4.8301078087305174e-05, 'epoch': 0.1}\n",
            "{'loss': 0.5363, 'learning_rate': 4.817972652211268e-05, 'epoch': 0.11}\n",
            "{'loss': 0.5555, 'learning_rate': 4.80583749569202e-05, 'epoch': 0.12}\n",
            "{'loss': 0.5462, 'learning_rate': 4.793702339172771e-05, 'epoch': 0.12}\n",
            "{'loss': 0.54, 'learning_rate': 4.781567182653522e-05, 'epoch': 0.13}\n",
            "{'loss': 0.546, 'learning_rate': 4.7694320261342736e-05, 'epoch': 0.14}\n",
            "{'loss': 0.5378, 'learning_rate': 4.757296869615024e-05, 'epoch': 0.15}\n",
            "{'loss': 0.5247, 'learning_rate': 4.7451617130957756e-05, 'epoch': 0.15}\n",
            "{'loss': 0.5459, 'learning_rate': 4.733026556576527e-05, 'epoch': 0.16}\n",
            "{'loss': 0.5363, 'learning_rate': 4.720891400057278e-05, 'epoch': 0.17}\n",
            "{'loss': 0.5252, 'learning_rate': 4.70875624353803e-05, 'epoch': 0.17}\n",
            "{'loss': 0.53, 'learning_rate': 4.6966210870187804e-05, 'epoch': 0.18}\n",
            "{'loss': 0.531, 'learning_rate': 4.684485930499532e-05, 'epoch': 0.19}\n",
            "{'loss': 0.4971, 'learning_rate': 4.672350773980283e-05, 'epoch': 0.2}\n",
            "{'loss': 0.504, 'learning_rate': 4.660215617461034e-05, 'epoch': 0.2}\n",
            "{'loss': 0.5122, 'learning_rate': 4.648080460941785e-05, 'epoch': 0.21}\n",
            "{'loss': 0.5198, 'learning_rate': 4.6359453044225366e-05, 'epoch': 0.22}\n",
            "{'loss': 0.5109, 'learning_rate': 4.623810147903288e-05, 'epoch': 0.23}\n",
            "{'loss': 0.508, 'learning_rate': 4.611674991384039e-05, 'epoch': 0.23}\n",
            "{'loss': 0.5033, 'learning_rate': 4.599539834864791e-05, 'epoch': 0.24}\n",
            "{'loss': 0.5171, 'learning_rate': 4.5874046783455414e-05, 'epoch': 0.25}\n",
            "{'loss': 0.5329, 'learning_rate': 4.575269521826293e-05, 'epoch': 0.25}\n",
            "{'loss': 0.509, 'learning_rate': 4.563134365307044e-05, 'epoch': 0.26}\n",
            "{'loss': 0.4946, 'learning_rate': 4.550999208787795e-05, 'epoch': 0.27}\n",
            "{'loss': 0.4922, 'learning_rate': 4.538864052268547e-05, 'epoch': 0.28}\n",
            "{'loss': 0.4954, 'learning_rate': 4.5267288957492976e-05, 'epoch': 0.28}\n",
            "{'loss': 0.5009, 'learning_rate': 4.514593739230048e-05, 'epoch': 0.29}\n",
            "{'loss': 0.486, 'learning_rate': 4.5024585827108e-05, 'epoch': 0.3}\n",
            "{'loss': 0.4947, 'learning_rate': 4.490323426191551e-05, 'epoch': 0.31}\n",
            "{'loss': 0.4946, 'learning_rate': 4.4781882696723024e-05, 'epoch': 0.31}\n",
            "{'loss': 0.4892, 'learning_rate': 4.466053113153054e-05, 'epoch': 0.32}\n",
            "{'loss': 0.5007, 'learning_rate': 4.4539179566338044e-05, 'epoch': 0.33}\n",
            "{'loss': 0.482, 'learning_rate': 4.4417828001145565e-05, 'epoch': 0.33}\n",
            "{'loss': 0.5005, 'learning_rate': 4.429647643595307e-05, 'epoch': 0.34}\n",
            "{'loss': 0.4953, 'learning_rate': 4.4175124870760585e-05, 'epoch': 0.35}\n",
            "{'loss': 0.48, 'learning_rate': 4.40537733055681e-05, 'epoch': 0.36}\n",
            "{'loss': 0.5082, 'learning_rate': 4.393242174037561e-05, 'epoch': 0.36}\n",
            "{'loss': 0.4854, 'learning_rate': 4.381107017518312e-05, 'epoch': 0.37}\n",
            "{'loss': 0.4916, 'learning_rate': 4.368971860999063e-05, 'epoch': 0.38}\n",
            "{'loss': 0.4969, 'learning_rate': 4.356836704479815e-05, 'epoch': 0.39}\n",
            "{'loss': 0.4695, 'learning_rate': 4.3447015479605654e-05, 'epoch': 0.39}\n",
            "{'loss': 0.4918, 'learning_rate': 4.3325663914413174e-05, 'epoch': 0.4}\n",
            "{'loss': 0.4818, 'learning_rate': 4.320431234922068e-05, 'epoch': 0.41}\n",
            "{'loss': 0.4827, 'learning_rate': 4.3082960784028195e-05, 'epoch': 0.42}\n",
            "{'loss': 0.4808, 'learning_rate': 4.296160921883571e-05, 'epoch': 0.42}\n",
            "{'loss': 0.5006, 'learning_rate': 4.2840257653643216e-05, 'epoch': 0.43}\n",
            "{'loss': 0.478, 'learning_rate': 4.2718906088450736e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4783, 'learning_rate': 4.259755452325824e-05, 'epoch': 0.44}\n",
            "{'loss': 0.4652, 'learning_rate': 4.247620295806575e-05, 'epoch': 0.45}\n",
            "{'loss': 0.4669, 'learning_rate': 4.235485139287327e-05, 'epoch': 0.46}\n",
            "{'loss': 0.4712, 'learning_rate': 4.223349982768078e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4771, 'learning_rate': 4.211214826248829e-05, 'epoch': 0.47}\n",
            "{'loss': 0.4744, 'learning_rate': 4.1990796697295805e-05, 'epoch': 0.48}\n",
            "{'loss': 0.461, 'learning_rate': 4.186944513210331e-05, 'epoch': 0.49}\n",
            "{'loss': 0.4748, 'learning_rate': 4.1748093566910825e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4778, 'learning_rate': 4.162674200171834e-05, 'epoch': 0.5}\n",
            "{'loss': 0.4606, 'learning_rate': 4.150539043652585e-05, 'epoch': 0.51}\n",
            "{'loss': 0.4752, 'learning_rate': 4.1384038871333367e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4769, 'learning_rate': 4.126268730614088e-05, 'epoch': 0.52}\n",
            "{'loss': 0.4573, 'learning_rate': 4.114133574094839e-05, 'epoch': 0.53}\n",
            "{'loss': 0.4529, 'learning_rate': 4.10199841757559e-05, 'epoch': 0.54}\n",
            "{'loss': 0.4838, 'learning_rate': 4.0898632610563415e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4757, 'learning_rate': 4.077728104537092e-05, 'epoch': 0.55}\n",
            "{'loss': 0.4563, 'learning_rate': 4.065592948017844e-05, 'epoch': 0.56}\n",
            "{'loss': 0.4811, 'learning_rate': 4.053457791498595e-05, 'epoch': 0.57}\n",
            "{'loss': 0.4594, 'learning_rate': 4.041322634979346e-05, 'epoch': 0.58}\n",
            "{'loss': 0.481, 'learning_rate': 4.0291874784600976e-05, 'epoch': 0.58}\n",
            "{'loss': 0.4469, 'learning_rate': 4.017052321940848e-05, 'epoch': 0.59}\n",
            "{'loss': 0.4537, 'learning_rate': 4.0049171654216e-05, 'epoch': 0.6}\n",
            "{'loss': 0.4372, 'learning_rate': 3.992782008902351e-05, 'epoch': 0.6}\n",
            "{'loss': 0.4513, 'learning_rate': 3.980646852383102e-05, 'epoch': 0.61}\n",
            "{'loss': 0.4521, 'learning_rate': 3.968511695863854e-05, 'epoch': 0.62}\n",
            "{'loss': 0.445, 'learning_rate': 3.9563765393446045e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4594, 'learning_rate': 3.944241382825356e-05, 'epoch': 0.63}\n",
            "{'loss': 0.4603, 'learning_rate': 3.932106226306107e-05, 'epoch': 0.64}\n",
            "{'loss': 0.4574, 'learning_rate': 3.9199710697868586e-05, 'epoch': 0.65}\n",
            "{'loss': 0.4468, 'learning_rate': 3.907835913267609e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4494, 'learning_rate': 3.8957007567483607e-05, 'epoch': 0.66}\n",
            "{'loss': 0.4525, 'learning_rate': 3.883565600229112e-05, 'epoch': 0.67}\n",
            "{'loss': 0.4551, 'learning_rate': 3.8714304437098634e-05, 'epoch': 0.68}\n",
            "{'loss': 0.4626, 'learning_rate': 3.859295287190615e-05, 'epoch': 0.68}\n",
            "{'loss': 0.4484, 'learning_rate': 3.8471601306713655e-05, 'epoch': 0.69}\n",
            "{'loss': 0.4521, 'learning_rate': 3.835024974152117e-05, 'epoch': 0.7}\n",
            "{'loss': 0.4526, 'learning_rate': 3.822889817632868e-05, 'epoch': 0.71}\n",
            "{'loss': 0.4511, 'learning_rate': 3.810754661113619e-05, 'epoch': 0.71}\n",
            "{'loss': 0.4239, 'learning_rate': 3.798619504594371e-05, 'epoch': 0.72}\n",
            "{'loss': 0.4558, 'learning_rate': 3.7864843480751216e-05, 'epoch': 0.73}\n",
            "{'loss': 0.4665, 'learning_rate': 3.774349191555872e-05, 'epoch': 0.74}\n",
            "{'loss': 0.4382, 'learning_rate': 3.7622140350366244e-05, 'epoch': 0.74}\n",
            "{'loss': 0.465, 'learning_rate': 3.750078878517375e-05, 'epoch': 0.75}\n",
            "{'loss': 0.4492, 'learning_rate': 3.7379437219981264e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4478, 'learning_rate': 3.725808565478878e-05, 'epoch': 0.76}\n",
            "{'loss': 0.4702, 'learning_rate': 3.7136734089596285e-05, 'epoch': 0.77}\n",
            "{'loss': 0.4654, 'learning_rate': 3.7015382524403805e-05, 'epoch': 0.78}\n",
            "{'loss': 0.4401, 'learning_rate': 3.689403095921131e-05, 'epoch': 0.79}\n",
            "{'loss': 0.4434, 'learning_rate': 3.6772679394018826e-05, 'epoch': 0.79}\n",
            "{'loss': 0.474, 'learning_rate': 3.665132782882634e-05, 'epoch': 0.8}\n",
            "{'loss': 0.453, 'learning_rate': 3.6529976263633853e-05, 'epoch': 0.81}\n",
            "{'loss': 0.4516, 'learning_rate': 3.640862469844136e-05, 'epoch': 0.82}\n",
            "{'loss': 0.4505, 'learning_rate': 3.6287273133248874e-05, 'epoch': 0.82}\n",
            "{'loss': 0.4316, 'learning_rate': 3.616592156805639e-05, 'epoch': 0.83}\n",
            "{'loss': 0.4427, 'learning_rate': 3.6044570002863895e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4526, 'learning_rate': 3.5923218437671415e-05, 'epoch': 0.84}\n",
            "{'loss': 0.4427, 'learning_rate': 3.580186687247892e-05, 'epoch': 0.85}\n",
            "{'loss': 0.4286, 'learning_rate': 3.5680515307286436e-05, 'epoch': 0.86}\n",
            "{'loss': 0.4415, 'learning_rate': 3.555916374209395e-05, 'epoch': 0.87}\n",
            "{'loss': 0.4635, 'learning_rate': 3.5437812176901456e-05, 'epoch': 0.87}\n",
            "{'loss': 0.447, 'learning_rate': 3.531646061170898e-05, 'epoch': 0.88}\n",
            "{'loss': 0.4449, 'learning_rate': 3.5195109046516484e-05, 'epoch': 0.89}\n",
            "{'loss': 0.4432, 'learning_rate': 3.507375748132399e-05, 'epoch': 0.9}\n",
            "{'loss': 0.4249, 'learning_rate': 3.495240591613151e-05, 'epoch': 0.9}\n",
            "{'loss': 0.4438, 'learning_rate': 3.483105435093902e-05, 'epoch': 0.91}\n",
            "{'loss': 0.4147, 'learning_rate': 3.470970278574653e-05, 'epoch': 0.92}\n",
            "{'loss': 0.4617, 'learning_rate': 3.4588351220554045e-05, 'epoch': 0.92}\n",
            "{'loss': 0.4206, 'learning_rate': 3.446699965536155e-05, 'epoch': 0.93}\n",
            "{'loss': 0.4402, 'learning_rate': 3.4345648090169066e-05, 'epoch': 0.94}\n",
            "{'loss': 0.4251, 'learning_rate': 3.422429652497658e-05, 'epoch': 0.95}\n",
            "{'loss': 0.4334, 'learning_rate': 3.4102944959784094e-05, 'epoch': 0.95}\n",
            "{'loss': 0.4373, 'learning_rate': 3.398159339459161e-05, 'epoch': 0.96}\n",
            "{'loss': 0.4411, 'learning_rate': 3.386024182939912e-05, 'epoch': 0.97}\n",
            "{'loss': 0.4327, 'learning_rate': 3.373889026420663e-05, 'epoch': 0.98}\n",
            "{'loss': 0.4416, 'learning_rate': 3.361753869901414e-05, 'epoch': 0.98}\n",
            "{'loss': 0.4461, 'learning_rate': 3.3496187133821655e-05, 'epoch': 0.99}\n",
            "{'loss': 0.4197, 'learning_rate': 3.337483556862916e-05, 'epoch': 1.0}\n",
            "{'loss': 0.4317, 'learning_rate': 3.325348400343668e-05, 'epoch': 1.0}\n",
            "{'loss': 0.3975, 'learning_rate': 3.313213243824419e-05, 'epoch': 1.01}\n",
            "{'loss': 0.4158, 'learning_rate': 3.30107808730517e-05, 'epoch': 1.02}\n",
            "{'loss': 0.4168, 'learning_rate': 3.288942930785922e-05, 'epoch': 1.03}\n",
            "{'loss': 0.404, 'learning_rate': 3.2768077742666724e-05, 'epoch': 1.03}\n",
            "{'loss': 0.4123, 'learning_rate': 3.264672617747424e-05, 'epoch': 1.04}\n",
            "{'loss': 0.4027, 'learning_rate': 3.252537461228175e-05, 'epoch': 1.05}\n",
            "{'loss': 0.4146, 'learning_rate': 3.240402304708926e-05, 'epoch': 1.06}\n",
            "{'loss': 0.4084, 'learning_rate': 3.228267148189678e-05, 'epoch': 1.06}\n",
            "{'loss': 0.4158, 'learning_rate': 3.2161319916704286e-05, 'epoch': 1.07}\n",
            "{'loss': 0.3962, 'learning_rate': 3.20399683515118e-05, 'epoch': 1.08}\n",
            "{'loss': 0.4011, 'learning_rate': 3.191861678631931e-05, 'epoch': 1.08}\n",
            "{'loss': 0.4114, 'learning_rate': 3.179726522112683e-05, 'epoch': 1.09}\n",
            "{'loss': 0.4075, 'learning_rate': 3.1675913655934334e-05, 'epoch': 1.1}\n",
            "{'loss': 0.4009, 'learning_rate': 3.155456209074185e-05, 'epoch': 1.11}\n",
            "{'loss': 0.4109, 'learning_rate': 3.143321052554936e-05, 'epoch': 1.11}\n",
            "{'loss': 0.4064, 'learning_rate': 3.131185896035687e-05, 'epoch': 1.12}\n",
            "{'loss': 0.4002, 'learning_rate': 3.119050739516439e-05, 'epoch': 1.13}\n",
            "{'loss': 0.4065, 'learning_rate': 3.1069155829971895e-05, 'epoch': 1.14}\n",
            "{'loss': 0.3981, 'learning_rate': 3.094780426477941e-05, 'epoch': 1.14}\n",
            "{'loss': 0.4219, 'learning_rate': 3.082645269958692e-05, 'epoch': 1.15}\n",
            "{'loss': 0.4004, 'learning_rate': 3.070510113439443e-05, 'epoch': 1.16}\n",
            "{'loss': 0.4202, 'learning_rate': 3.058374956920195e-05, 'epoch': 1.16}\n",
            "{'loss': 0.4014, 'learning_rate': 3.0462398004009457e-05, 'epoch': 1.17}\n",
            "{'loss': 0.3945, 'learning_rate': 3.0341046438816967e-05, 'epoch': 1.18}\n",
            "{'loss': 0.4145, 'learning_rate': 3.021969487362448e-05, 'epoch': 1.19}\n",
            "{'loss': 0.4106, 'learning_rate': 3.009834330843199e-05, 'epoch': 1.19}\n",
            "{'loss': 0.4213, 'learning_rate': 2.997699174323951e-05, 'epoch': 1.2}\n",
            "{'loss': 0.3981, 'learning_rate': 2.985564017804702e-05, 'epoch': 1.21}\n",
            "{'loss': 0.4044, 'learning_rate': 2.9734288612854526e-05, 'epoch': 1.22}\n",
            "{'loss': 0.412, 'learning_rate': 2.9612937047662043e-05, 'epoch': 1.22}\n",
            "{'loss': 0.3961, 'learning_rate': 2.9491585482469553e-05, 'epoch': 1.23}\n",
            "{'loss': 0.4125, 'learning_rate': 2.9370233917277067e-05, 'epoch': 1.24}\n",
            "{'loss': 0.406, 'learning_rate': 2.9248882352084577e-05, 'epoch': 1.25}\n",
            "{'loss': 0.4164, 'learning_rate': 2.9127530786892094e-05, 'epoch': 1.25}\n",
            "{'loss': 0.4238, 'learning_rate': 2.9006179221699604e-05, 'epoch': 1.26}\n",
            "{'loss': 0.4025, 'learning_rate': 2.888482765650711e-05, 'epoch': 1.27}\n",
            "{'loss': 0.4043, 'learning_rate': 2.876347609131463e-05, 'epoch': 1.27}\n",
            "{'loss': 0.3794, 'learning_rate': 2.864212452612214e-05, 'epoch': 1.28}\n",
            "{'loss': 0.3988, 'learning_rate': 2.8520772960929652e-05, 'epoch': 1.29}\n",
            "{'loss': 0.4126, 'learning_rate': 2.8399421395737163e-05, 'epoch': 1.3}\n",
            "{'loss': 0.3797, 'learning_rate': 2.8278069830544673e-05, 'epoch': 1.3}\n",
            "{'loss': 0.4065, 'learning_rate': 2.815671826535219e-05, 'epoch': 1.31}\n",
            "{'loss': 0.4165, 'learning_rate': 2.8035366700159697e-05, 'epoch': 1.32}\n",
            "{'loss': 0.4166, 'learning_rate': 2.7914015134967214e-05, 'epoch': 1.33}\n",
            "{'loss': 0.4166, 'learning_rate': 2.7792663569774724e-05, 'epoch': 1.33}\n",
            "{'loss': 0.3937, 'learning_rate': 2.7671312004582235e-05, 'epoch': 1.34}\n",
            "{'loss': 0.3978, 'learning_rate': 2.754996043938975e-05, 'epoch': 1.35}\n",
            "{'loss': 0.3943, 'learning_rate': 2.742860887419726e-05, 'epoch': 1.35}\n",
            "{'loss': 0.4015, 'learning_rate': 2.7307257309004776e-05, 'epoch': 1.36}\n",
            "{'loss': 0.4007, 'learning_rate': 2.7185905743812283e-05, 'epoch': 1.37}\n",
            "{'loss': 0.3962, 'learning_rate': 2.70645541786198e-05, 'epoch': 1.38}\n",
            "{'loss': 0.3994, 'learning_rate': 2.694320261342731e-05, 'epoch': 1.38}\n",
            "{'loss': 0.3996, 'learning_rate': 2.682185104823482e-05, 'epoch': 1.39}\n",
            "{'loss': 0.4153, 'learning_rate': 2.6700499483042334e-05, 'epoch': 1.4}\n",
            "{'loss': 0.3922, 'learning_rate': 2.6579147917849845e-05, 'epoch': 1.41}\n",
            "{'loss': 0.4042, 'learning_rate': 2.645779635265736e-05, 'epoch': 1.41}\n",
            "{'loss': 0.4079, 'learning_rate': 2.633644478746487e-05, 'epoch': 1.42}\n",
            "{'loss': 0.3899, 'learning_rate': 2.621509322227238e-05, 'epoch': 1.43}\n",
            "{'loss': 0.4111, 'learning_rate': 2.6093741657079896e-05, 'epoch': 1.43}\n",
            "{'loss': 0.3935, 'learning_rate': 2.5972390091887406e-05, 'epoch': 1.44}\n",
            "{'loss': 0.4202, 'learning_rate': 2.585103852669492e-05, 'epoch': 1.45}\n",
            "{'loss': 0.3925, 'learning_rate': 2.572968696150243e-05, 'epoch': 1.46}\n",
            "{'loss': 0.4114, 'learning_rate': 2.560833539630994e-05, 'epoch': 1.46}\n",
            "{'loss': 0.4262, 'learning_rate': 2.5486983831117454e-05, 'epoch': 1.47}\n",
            "{'loss': 0.3747, 'learning_rate': 2.5365632265924965e-05, 'epoch': 1.48}\n",
            "{'loss': 0.417, 'learning_rate': 2.524428070073248e-05, 'epoch': 1.49}\n",
            "{'loss': 0.4091, 'learning_rate': 2.5122929135539992e-05, 'epoch': 1.49}\n",
            "{'loss': 0.4052, 'learning_rate': 2.5001577570347502e-05, 'epoch': 1.5}\n",
            "{'loss': 0.4161, 'learning_rate': 2.4880226005155016e-05, 'epoch': 1.51}\n",
            "{'loss': 0.378, 'learning_rate': 2.475887443996253e-05, 'epoch': 1.51}\n",
            "{'loss': 0.4319, 'learning_rate': 2.463752287477004e-05, 'epoch': 1.52}\n",
            "{'loss': 0.4086, 'learning_rate': 2.451617130957755e-05, 'epoch': 1.53}\n",
            "{'loss': 0.4008, 'learning_rate': 2.4394819744385064e-05, 'epoch': 1.54}\n",
            "{'loss': 0.3896, 'learning_rate': 2.4273468179192578e-05, 'epoch': 1.54}\n",
            "{'loss': 0.3784, 'learning_rate': 2.4152116614000088e-05, 'epoch': 1.55}\n",
            "{'loss': 0.4177, 'learning_rate': 2.40307650488076e-05, 'epoch': 1.56}\n",
            "{'loss': 0.3926, 'learning_rate': 2.3909413483615112e-05, 'epoch': 1.57}\n",
            "{'loss': 0.384, 'learning_rate': 2.3788061918422626e-05, 'epoch': 1.57}\n",
            "{'loss': 0.4054, 'learning_rate': 2.3666710353230136e-05, 'epoch': 1.58}\n",
            "{'loss': 0.3997, 'learning_rate': 2.354535878803765e-05, 'epoch': 1.59}\n",
            "{'loss': 0.3976, 'learning_rate': 2.3424007222845163e-05, 'epoch': 1.59}\n",
            "{'loss': 0.3647, 'learning_rate': 2.3302655657652674e-05, 'epoch': 1.6}\n",
            "{'loss': 0.4009, 'learning_rate': 2.3181304092460184e-05, 'epoch': 1.61}\n",
            "{'loss': 0.4017, 'learning_rate': 2.3059952527267698e-05, 'epoch': 1.62}\n",
            "{'loss': 0.3816, 'learning_rate': 2.293860096207521e-05, 'epoch': 1.62}\n",
            "{'loss': 0.3693, 'learning_rate': 2.2817249396882722e-05, 'epoch': 1.63}\n",
            "{'loss': 0.3989, 'learning_rate': 2.2695897831690235e-05, 'epoch': 1.64}\n",
            "{'loss': 0.3895, 'learning_rate': 2.2574546266497746e-05, 'epoch': 1.65}\n",
            "{'loss': 0.3759, 'learning_rate': 2.245319470130526e-05, 'epoch': 1.65}\n",
            "{'loss': 0.3781, 'learning_rate': 2.233184313611277e-05, 'epoch': 1.66}\n",
            "{'loss': 0.3831, 'learning_rate': 2.2210491570920283e-05, 'epoch': 1.67}\n",
            "{'loss': 0.3928, 'learning_rate': 2.2089140005727797e-05, 'epoch': 1.67}\n",
            "{'loss': 0.3779, 'learning_rate': 2.1967788440535304e-05, 'epoch': 1.68}\n",
            "{'loss': 0.3899, 'learning_rate': 2.1846436875342818e-05, 'epoch': 1.69}\n",
            "{'loss': 0.3901, 'learning_rate': 2.172508531015033e-05, 'epoch': 1.7}\n",
            "{'loss': 0.3879, 'learning_rate': 2.1603733744957845e-05, 'epoch': 1.7}\n",
            "{'loss': 0.4036, 'learning_rate': 2.1482382179765355e-05, 'epoch': 1.71}\n",
            "{'loss': 0.4015, 'learning_rate': 2.136103061457287e-05, 'epoch': 1.72}\n",
            "{'loss': 0.3898, 'learning_rate': 2.123967904938038e-05, 'epoch': 1.73}\n",
            "{'loss': 0.3773, 'learning_rate': 2.111832748418789e-05, 'epoch': 1.73}\n",
            "{'loss': 0.4225, 'learning_rate': 2.0996975918995403e-05, 'epoch': 1.74}\n",
            "{'loss': 0.3697, 'learning_rate': 2.0875624353802917e-05, 'epoch': 1.75}\n",
            "{'loss': 0.3754, 'learning_rate': 2.075427278861043e-05, 'epoch': 1.75}\n",
            "{'loss': 0.385, 'learning_rate': 2.063292122341794e-05, 'epoch': 1.76}\n",
            "{'loss': 0.3965, 'learning_rate': 2.051156965822545e-05, 'epoch': 1.77}\n",
            "{'loss': 0.3862, 'learning_rate': 2.0390218093032965e-05, 'epoch': 1.78}\n",
            "{'loss': 0.4062, 'learning_rate': 2.0268866527840475e-05, 'epoch': 1.78}\n",
            "{'loss': 0.3837, 'learning_rate': 2.014751496264799e-05, 'epoch': 1.79}\n",
            "{'loss': 0.4027, 'learning_rate': 2.0026163397455503e-05, 'epoch': 1.8}\n",
            "{'loss': 0.366, 'learning_rate': 1.9904811832263013e-05, 'epoch': 1.81}\n",
            "{'loss': 0.3998, 'learning_rate': 1.9783460267070523e-05, 'epoch': 1.81}\n",
            "{'loss': 0.3888, 'learning_rate': 1.9662108701878037e-05, 'epoch': 1.82}\n",
            "{'loss': 0.3968, 'learning_rate': 1.954075713668555e-05, 'epoch': 1.83}\n",
            "{'loss': 0.394, 'learning_rate': 1.941940557149306e-05, 'epoch': 1.83}\n",
            "{'loss': 0.3649, 'learning_rate': 1.9298054006300575e-05, 'epoch': 1.84}\n",
            "{'loss': 0.3876, 'learning_rate': 1.9176702441108085e-05, 'epoch': 1.85}\n",
            "{'loss': 0.3748, 'learning_rate': 1.90553508759156e-05, 'epoch': 1.86}\n",
            "{'loss': 0.3712, 'learning_rate': 1.893399931072311e-05, 'epoch': 1.86}\n",
            "{'loss': 0.4066, 'learning_rate': 1.8812647745530623e-05, 'epoch': 1.87}\n",
            "{'loss': 0.3924, 'learning_rate': 1.8691296180338137e-05, 'epoch': 1.88}\n",
            "{'loss': 0.386, 'learning_rate': 1.8569944615145647e-05, 'epoch': 1.89}\n",
            "{'loss': 0.3851, 'learning_rate': 1.8448593049953157e-05, 'epoch': 1.89}\n",
            "{'loss': 0.3869, 'learning_rate': 1.832724148476067e-05, 'epoch': 1.9}\n",
            "{'loss': 0.3763, 'learning_rate': 1.8205889919568185e-05, 'epoch': 1.91}\n",
            "{'loss': 0.3998, 'learning_rate': 1.8084538354375695e-05, 'epoch': 1.91}\n",
            "{'loss': 0.3985, 'learning_rate': 1.796318678918321e-05, 'epoch': 1.92}\n",
            "{'loss': 0.3687, 'learning_rate': 1.784183522399072e-05, 'epoch': 1.93}\n",
            "{'loss': 0.3719, 'learning_rate': 1.7720483658798233e-05, 'epoch': 1.94}\n",
            "{'loss': 0.3936, 'learning_rate': 1.7599132093605743e-05, 'epoch': 1.94}\n",
            "{'loss': 0.3875, 'learning_rate': 1.7477780528413257e-05, 'epoch': 1.95}\n",
            "{'loss': 0.401, 'learning_rate': 1.735642896322077e-05, 'epoch': 1.96}\n",
            "{'loss': 0.3933, 'learning_rate': 1.723507739802828e-05, 'epoch': 1.97}\n",
            "{'loss': 0.3814, 'learning_rate': 1.711372583283579e-05, 'epoch': 1.97}\n",
            "{'loss': 0.3853, 'learning_rate': 1.6992374267643305e-05, 'epoch': 1.98}\n",
            "{'loss': 0.3746, 'learning_rate': 1.687102270245082e-05, 'epoch': 1.99}\n",
            "{'loss': 0.3813, 'learning_rate': 1.674967113725833e-05, 'epoch': 2.0}\n",
            "{'loss': 0.3651, 'learning_rate': 1.6628319572065842e-05, 'epoch': 2.0}\n",
            "{'loss': 0.3536, 'learning_rate': 1.6506968006873353e-05, 'epoch': 2.01}\n",
            "{'loss': 0.3615, 'learning_rate': 1.6385616441680866e-05, 'epoch': 2.02}\n",
            "{'loss': 0.3354, 'learning_rate': 1.6264264876488377e-05, 'epoch': 2.02}\n",
            "{'loss': 0.3494, 'learning_rate': 1.614291331129589e-05, 'epoch': 2.03}\n",
            "{'loss': 0.3517, 'learning_rate': 1.6021561746103404e-05, 'epoch': 2.04}\n",
            "{'loss': 0.3609, 'learning_rate': 1.5900210180910914e-05, 'epoch': 2.05}\n",
            "{'loss': 0.3545, 'learning_rate': 1.5778858615718425e-05, 'epoch': 2.05}\n",
            "{'loss': 0.3623, 'learning_rate': 1.565750705052594e-05, 'epoch': 2.06}\n",
            "{'loss': 0.3618, 'learning_rate': 1.5536155485333452e-05, 'epoch': 2.07}\n",
            "{'loss': 0.3295, 'learning_rate': 1.5414803920140962e-05, 'epoch': 2.08}\n",
            "{'loss': 0.3614, 'learning_rate': 1.5293452354948476e-05, 'epoch': 2.08}\n",
            "{'loss': 0.3546, 'learning_rate': 1.5172100789755986e-05, 'epoch': 2.09}\n",
            "{'loss': 0.3417, 'learning_rate': 1.5050749224563498e-05, 'epoch': 2.1}\n",
            "{'loss': 0.3361, 'learning_rate': 1.492939765937101e-05, 'epoch': 2.1}\n",
            "{'loss': 0.3675, 'learning_rate': 1.4808046094178524e-05, 'epoch': 2.11}\n",
            "{'loss': 0.3662, 'learning_rate': 1.4686694528986036e-05, 'epoch': 2.12}\n",
            "{'loss': 0.3631, 'learning_rate': 1.456534296379355e-05, 'epoch': 2.13}\n",
            "{'loss': 0.345, 'learning_rate': 1.4443991398601058e-05, 'epoch': 2.13}\n",
            "{'loss': 0.3531, 'learning_rate': 1.4322639833408572e-05, 'epoch': 2.14}\n",
            "{'loss': 0.3656, 'learning_rate': 1.4201288268216084e-05, 'epoch': 2.15}\n",
            "{'loss': 0.3659, 'learning_rate': 1.4079936703023596e-05, 'epoch': 2.16}\n",
            "{'loss': 0.3334, 'learning_rate': 1.395858513783111e-05, 'epoch': 2.16}\n",
            "{'loss': 0.3511, 'learning_rate': 1.383723357263862e-05, 'epoch': 2.17}\n",
            "{'loss': 0.3486, 'learning_rate': 1.3715882007446132e-05, 'epoch': 2.18}\n",
            "{'loss': 0.3241, 'learning_rate': 1.3594530442253644e-05, 'epoch': 2.18}\n",
            "{'loss': 0.3658, 'learning_rate': 1.3473178877061158e-05, 'epoch': 2.19}\n",
            "{'loss': 0.364, 'learning_rate': 1.335182731186867e-05, 'epoch': 2.2}\n",
            "{'loss': 0.3647, 'learning_rate': 1.3230475746676182e-05, 'epoch': 2.21}\n",
            "{'loss': 0.3475, 'learning_rate': 1.3109124181483692e-05, 'epoch': 2.21}\n",
            "{'loss': 0.3503, 'learning_rate': 1.2987772616291204e-05, 'epoch': 2.22}\n",
            "{'loss': 0.3454, 'learning_rate': 1.2866421051098718e-05, 'epoch': 2.23}\n",
            "{'loss': 0.3643, 'learning_rate': 1.274506948590623e-05, 'epoch': 2.24}\n",
            "{'loss': 0.3447, 'learning_rate': 1.2623717920713744e-05, 'epoch': 2.24}\n",
            "{'loss': 0.3564, 'learning_rate': 1.2502366355521252e-05, 'epoch': 2.25}\n",
            "{'loss': 0.3618, 'learning_rate': 1.2381014790328768e-05, 'epoch': 2.26}\n",
            "{'loss': 0.3623, 'learning_rate': 1.2259663225136278e-05, 'epoch': 2.26}\n",
            "{'loss': 0.3556, 'learning_rate': 1.213831165994379e-05, 'epoch': 2.27}\n",
            "{'loss': 0.3517, 'learning_rate': 1.2016960094751304e-05, 'epoch': 2.28}\n",
            "{'loss': 0.365, 'learning_rate': 1.1895608529558814e-05, 'epoch': 2.29}\n",
            "{'loss': 0.3516, 'learning_rate': 1.1774256964366328e-05, 'epoch': 2.29}\n",
            "{'loss': 0.3376, 'learning_rate': 1.1652905399173838e-05, 'epoch': 2.3}\n",
            "{'loss': 0.3391, 'learning_rate': 1.1531553833981352e-05, 'epoch': 2.31}\n",
            "{'loss': 0.3631, 'learning_rate': 1.1410202268788864e-05, 'epoch': 2.32}\n",
            "{'loss': 0.3667, 'learning_rate': 1.1288850703596376e-05, 'epoch': 2.32}\n",
            "{'loss': 0.3535, 'learning_rate': 1.1167499138403888e-05, 'epoch': 2.33}\n",
            "{'loss': 0.3544, 'learning_rate': 1.10461475732114e-05, 'epoch': 2.34}\n",
            "{'loss': 0.3414, 'learning_rate': 1.0924796008018912e-05, 'epoch': 2.34}\n",
            "{'loss': 0.3531, 'learning_rate': 1.0803444442826424e-05, 'epoch': 2.35}\n",
            "{'loss': 0.3234, 'learning_rate': 1.0682092877633937e-05, 'epoch': 2.36}\n",
            "{'loss': 0.3461, 'learning_rate': 1.0560741312441448e-05, 'epoch': 2.37}\n",
            "{'loss': 0.3279, 'learning_rate': 1.0439389747248961e-05, 'epoch': 2.37}\n",
            "{'loss': 0.3537, 'learning_rate': 1.0318038182056472e-05, 'epoch': 2.38}\n",
            "{'loss': 0.3645, 'learning_rate': 1.0196686616863985e-05, 'epoch': 2.39}\n",
            "{'loss': 0.349, 'learning_rate': 1.0075335051671497e-05, 'epoch': 2.4}\n",
            "{'loss': 0.3561, 'learning_rate': 9.95398348647901e-06, 'epoch': 2.4}\n",
            "{'loss': 0.3526, 'learning_rate': 9.832631921286521e-06, 'epoch': 2.41}\n",
            "{'loss': 0.3485, 'learning_rate': 9.711280356094033e-06, 'epoch': 2.42}\n",
            "{'loss': 0.3431, 'learning_rate': 9.589928790901545e-06, 'epoch': 2.42}\n",
            "{'loss': 0.3296, 'learning_rate': 9.468577225709057e-06, 'epoch': 2.43}\n",
            "{'loss': 0.3651, 'learning_rate': 9.347225660516571e-06, 'epoch': 2.44}\n",
            "{'loss': 0.3632, 'learning_rate': 9.225874095324081e-06, 'epoch': 2.45}\n",
            "{'loss': 0.3537, 'learning_rate': 9.104522530131595e-06, 'epoch': 2.45}\n",
            "{'loss': 0.3632, 'learning_rate': 8.983170964939107e-06, 'epoch': 2.46}\n",
            "{'loss': 0.3632, 'learning_rate': 8.861819399746617e-06, 'epoch': 2.47}\n",
            "{'loss': 0.359, 'learning_rate': 8.740467834554131e-06, 'epoch': 2.48}\n",
            "{'loss': 0.338, 'learning_rate': 8.619116269361641e-06, 'epoch': 2.48}\n",
            "{'loss': 0.3419, 'learning_rate': 8.497764704169155e-06, 'epoch': 2.49}\n",
            "{'loss': 0.356, 'learning_rate': 8.376413138976667e-06, 'epoch': 2.5}\n",
            "{'loss': 0.3485, 'learning_rate': 8.255061573784179e-06, 'epoch': 2.5}\n",
            "{'loss': 0.3786, 'learning_rate': 8.133710008591691e-06, 'epoch': 2.51}\n",
            "{'loss': 0.3154, 'learning_rate': 8.012358443399203e-06, 'epoch': 2.52}\n",
            "{'loss': 0.3539, 'learning_rate': 7.891006878206715e-06, 'epoch': 2.53}\n",
            "{'loss': 0.3479, 'learning_rate': 7.769655313014227e-06, 'epoch': 2.53}\n",
            "{'loss': 0.3483, 'learning_rate': 7.64830374782174e-06, 'epoch': 2.54}\n",
            "{'loss': 0.3498, 'learning_rate': 7.526952182629252e-06, 'epoch': 2.55}\n",
            "{'loss': 0.345, 'learning_rate': 7.405600617436764e-06, 'epoch': 2.56}\n",
            "{'loss': 0.3365, 'learning_rate': 7.284249052244275e-06, 'epoch': 2.56}\n",
            "{'loss': 0.3531, 'learning_rate': 7.162897487051788e-06, 'epoch': 2.57}\n",
            "{'loss': 0.3415, 'learning_rate': 7.041545921859301e-06, 'epoch': 2.58}\n",
            "{'loss': 0.3773, 'learning_rate': 6.920194356666812e-06, 'epoch': 2.58}\n",
            "{'loss': 0.3452, 'learning_rate': 6.798842791474325e-06, 'epoch': 2.59}\n",
            "{'loss': 0.3301, 'learning_rate': 6.677491226281838e-06, 'epoch': 2.6}\n",
            "{'loss': 0.3374, 'learning_rate': 6.556139661089349e-06, 'epoch': 2.61}\n",
            "{'loss': 0.3312, 'learning_rate': 6.434788095896861e-06, 'epoch': 2.61}\n",
            "{'loss': 0.3533, 'learning_rate': 6.313436530704374e-06, 'epoch': 2.62}\n",
            "{'loss': 0.3447, 'learning_rate': 6.192084965511886e-06, 'epoch': 2.63}\n",
            "{'loss': 0.3287, 'learning_rate': 6.070733400319398e-06, 'epoch': 2.64}\n",
            "{'loss': 0.3372, 'learning_rate': 5.94938183512691e-06, 'epoch': 2.64}\n",
            "{'loss': 0.3352, 'learning_rate': 5.828030269934422e-06, 'epoch': 2.65}\n",
            "{'loss': 0.3687, 'learning_rate': 5.7066787047419345e-06, 'epoch': 2.66}\n",
            "{'loss': 0.3522, 'learning_rate': 5.5853271395494466e-06, 'epoch': 2.66}\n",
            "{'loss': 0.3314, 'learning_rate': 5.4639755743569586e-06, 'epoch': 2.67}\n",
            "{'loss': 0.3254, 'learning_rate': 5.34262400916447e-06, 'epoch': 2.68}\n",
            "{'loss': 0.3596, 'learning_rate': 5.2212724439719826e-06, 'epoch': 2.69}\n",
            "{'loss': 0.3275, 'learning_rate': 5.0999208787794946e-06, 'epoch': 2.69}\n",
            "{'loss': 0.338, 'learning_rate': 4.9785693135870066e-06, 'epoch': 2.7}\n",
            "{'loss': 0.3311, 'learning_rate': 4.857217748394519e-06, 'epoch': 2.71}\n",
            "{'loss': 0.3607, 'learning_rate': 4.7358661832020314e-06, 'epoch': 2.72}\n",
            "{'loss': 0.3542, 'learning_rate': 4.6145146180095434e-06, 'epoch': 2.72}\n",
            "{'loss': 0.343, 'learning_rate': 4.4931630528170554e-06, 'epoch': 2.73}\n",
            "{'loss': 0.3743, 'learning_rate': 4.3718114876245674e-06, 'epoch': 2.74}\n",
            "{'loss': 0.3599, 'learning_rate': 4.2504599224320794e-06, 'epoch': 2.74}\n",
            "{'loss': 0.3273, 'learning_rate': 4.1291083572395914e-06, 'epoch': 2.75}\n",
            "{'loss': 0.3318, 'learning_rate': 4.007756792047104e-06, 'epoch': 2.76}\n",
            "{'loss': 0.3509, 'learning_rate': 3.886405226854616e-06, 'epoch': 2.77}\n",
            "{'loss': 0.3536, 'learning_rate': 3.7650536616621283e-06, 'epoch': 2.77}\n",
            "{'loss': 0.348, 'learning_rate': 3.6437020964696403e-06, 'epoch': 2.78}\n",
            "{'loss': 0.3432, 'learning_rate': 3.5223505312771527e-06, 'epoch': 2.79}\n",
            "{'loss': 0.3271, 'learning_rate': 3.4009989660846647e-06, 'epoch': 2.8}\n",
            "{'loss': 0.3208, 'learning_rate': 3.2796474008921767e-06, 'epoch': 2.8}\n",
            "{'loss': 0.3315, 'learning_rate': 3.158295835699689e-06, 'epoch': 2.81}\n",
            "{'loss': 0.3705, 'learning_rate': 3.036944270507201e-06, 'epoch': 2.82}\n",
            "{'loss': 0.3263, 'learning_rate': 2.915592705314713e-06, 'epoch': 2.83}\n",
            "{'loss': 0.3346, 'learning_rate': 2.7942411401222256e-06, 'epoch': 2.83}\n",
            "{'loss': 0.3419, 'learning_rate': 2.6728895749297376e-06, 'epoch': 2.84}\n",
            "{'loss': 0.3163, 'learning_rate': 2.5515380097372496e-06, 'epoch': 2.85}\n",
            "{'loss': 0.3731, 'learning_rate': 2.4301864445447616e-06, 'epoch': 2.85}\n",
            "{'loss': 0.3401, 'learning_rate': 2.308834879352274e-06, 'epoch': 2.86}\n",
            "{'loss': 0.3683, 'learning_rate': 2.1874833141597865e-06, 'epoch': 2.87}\n",
            "{'loss': 0.3135, 'learning_rate': 2.066131748967298e-06, 'epoch': 2.88}\n",
            "{'loss': 0.3446, 'learning_rate': 1.9447801837748105e-06, 'epoch': 2.88}\n",
            "{'loss': 0.3267, 'learning_rate': 1.8234286185823225e-06, 'epoch': 2.89}\n",
            "{'loss': 0.3534, 'learning_rate': 1.7020770533898347e-06, 'epoch': 2.9}\n",
            "{'loss': 0.3163, 'learning_rate': 1.5807254881973467e-06, 'epoch': 2.91}\n",
            "{'loss': 0.3773, 'learning_rate': 1.459373923004859e-06, 'epoch': 2.91}\n",
            "{'loss': 0.3515, 'learning_rate': 1.3380223578123711e-06, 'epoch': 2.92}\n",
            "{'loss': 0.3524, 'learning_rate': 1.2166707926198831e-06, 'epoch': 2.93}\n",
            "{'loss': 0.3385, 'learning_rate': 1.0953192274273953e-06, 'epoch': 2.93}\n",
            "{'loss': 0.3367, 'learning_rate': 9.739676622349076e-07, 'epoch': 2.94}\n",
            "{'loss': 0.3335, 'learning_rate': 8.526160970424198e-07, 'epoch': 2.95}\n",
            "{'loss': 0.3294, 'learning_rate': 7.312645318499318e-07, 'epoch': 2.96}\n",
            "{'loss': 0.3466, 'learning_rate': 6.09912966657444e-07, 'epoch': 2.96}\n",
            "{'loss': 0.337, 'learning_rate': 4.885614014649561e-07, 'epoch': 2.97}\n",
            "{'loss': 0.3528, 'learning_rate': 3.6720983627246827e-07, 'epoch': 2.98}\n",
            "{'loss': 0.3413, 'learning_rate': 2.4585827107998043e-07, 'epoch': 2.99}\n",
            "{'loss': 0.3637, 'learning_rate': 1.2450670588749254e-07, 'epoch': 2.99}\n",
            "{'loss': 0.3262, 'learning_rate': 3.155140695004684e-09, 'epoch': 3.0}\n",
            "{'train_runtime': 9672.6527, 'train_samples_per_second': 170.388, 'train_steps_per_second': 21.299, 'train_loss': 0.4117130315570039, 'epoch': 3.0}\n",
            "100% 206013/206013 [2:41:12<00:00, 21.30it/s]\n"
          ]
        }
      ],
      "source": [
        "! python3 /content/NLP_FP/nlp_fp/fp-dataset-artifacts/run.py --do_train --task nli --dataset snli --output_dir ./trained_model/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "oFavcTdqKldI",
        "outputId": "6c3e92be-3bde-4081-cab5-bc711986d3e9"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4cf671fd-7b7e-4e8f-bbd6-503b85250410\", \"checkpoint-206000.zip\", 134381678)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# save the checkpoint for future evaluation\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "folder_path = '/content/NLP_FP/nlp_fp/trained_model/checkpoint-206000'\n",
        "zip_path = '/content/NLP_FP/nlp_fp/trained_model/checkpoint-206000.zip'\n",
        "\n",
        "shutil.make_archive(zip_path.replace('.zip', ''), 'zip', folder_path)\n",
        "from google.colab import files\n",
        "files.download(zip_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIh5_6j_t_sx"
      },
      "source": [
        "## Evaluate the pretrained model on the original snli test dataset, got 89.20 accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKy4hOrNVNkN"
      },
      "outputs": [],
      "source": [
        "# !pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WvneyyT3Qqn",
        "outputId": "d78664f1-1508-4d69-cce1-f1e5fa8a976b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-11-19 05:42:15.588075: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-19 05:42:15.588131: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-19 05:42:15.588168: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-19 05:42:17.804095: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Map (num_proc=2): 100% 9842/9842 [00:01<00:00, 5903.16 examples/s]\n",
            "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 1231/1231 [00:19<00:00, 64.48it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 0.3803400695323944, 'eval_accuracy': 0.891993522644043, 'eval_runtime': 19.7523, 'eval_samples_per_second': 498.27, 'eval_steps_per_second': 62.322}\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/NLP_FP/nlp_fp/fp-dataset-artifacts/run.py --do_eval --task nli --dataset snli --model /content/NLP_FP/nlp_fp/trained_model/checkpoint --output_dir ./eval_output/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkFa2i3yV0j5",
        "outputId": "bbb42c0d-b1f1-4aeb-8181-2947f3b1c5ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-12-02 02:25:53.179345: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-12-02 02:25:53.179411: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-12-02 02:25:53.179471: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-12-02 02:25:54.486397: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "Downloading data files: 100% 1/1 [00:00<00:00, 9341.43it/s]\n",
            "Extracting data files: 100% 1/1 [00:00<00:00, 1462.45it/s]\n",
            "Generating train split: 93564 examples [00:00, 104616.56 examples/s]\n",
            "Preprocessing data... (this takes a little bit, should only happen once per dataset)\n",
            "Map (num_proc=2): 100% 93564/93564 [00:17<00:00, 5358.53 examples/s]\n",
            "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "100% 11696/11696 [03:12<00:00, 60.81it/s]\n",
            "Evaluation results:\n",
            "{'eval_loss': 1.342334508895874, 'eval_accuracy': 0.6349343657493591, 'eval_runtime': 192.969, 'eval_samples_per_second': 484.865, 'eval_steps_per_second': 60.611}\n"
          ]
        }
      ],
      "source": [
        "!python3 /content/NLP_FP/nlp_fp/fp-dataset-artifacts/run.py --do_eval --task nli --dataset /content/NLP_FP/nlp_fp/transfer/clean_dataset/process_use_contrast.json --model /content/NLP_FP/nlp_fp/trained_model/checkpoint --output_dir /content/NLP_FP/nlp_fp/contrast_output_v2/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIqHpcEy2pIl",
        "outputId": "4e4ab5f2-6de1-4490-ef5a-1d7dc1d3db7d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting textattack\n",
            "  Downloading textattack-0.3.9-py3-none-any.whl (436 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.8/436.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bert-score>=0.3.5 (from textattack)\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: editdistance in /usr/local/lib/python3.10/dist-packages (from textattack) (0.6.2)\n",
            "Collecting flair (from textattack)\n",
            "  Downloading flair-0.13.0-py3-none-any.whl (387 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m387.2/387.2 kB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from textattack) (3.13.1)\n",
            "Collecting language-tool-python (from textattack)\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Collecting lemminflect (from textattack)\n",
            "  Downloading lemminflect-0.2.3-py3-none-any.whl (769 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m769.7/769.7 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting lru-dict (from textattack)\n",
            "  Downloading lru_dict-1.3.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: datasets>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textattack) (3.8.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.23.5)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.5.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.11.4)\n",
            "Requirement already satisfied: torch!=1.8,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (2.1.0+cu118)\n",
            "Requirement already satisfied: transformers>=4.30.0 in /usr/local/lib/python3.10/dist-packages (from textattack) (4.35.2)\n",
            "Collecting terminaltables (from textattack)\n",
            "  Downloading terminaltables-3.1.10-py2.py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from textattack) (4.66.1)\n",
            "Collecting word2number (from textattack)\n",
            "  Downloading word2number-1.1.zip (9.7 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting num2words (from textattack)\n",
            "  Downloading num2words-0.5.13-py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.3/143.3 kB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from textattack) (10.1.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.10/dist-packages (from textattack) (1.7.1)\n",
            "Collecting pinyin>=0.4.0 (from textattack)\n",
            "  Downloading pinyin-0.4.0.tar.gz (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: jieba in /usr/local/lib/python3.10/dist-packages (from textattack) (0.42.1)\n",
            "Collecting OpenHowNet (from textattack)\n",
            "  Downloading OpenHowNet-2.0-py3-none-any.whl (18 kB)\n",
            "Collecting pycld2 (from textattack)\n",
            "  Downloading pycld2-0.41.tar.gz (41.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.4/41.4 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting click<8.1.0 (from textattack)\n",
            "  Downloading click-8.0.4-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (2.31.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (3.7.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from bert-score>=0.3.5->textattack) (23.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (9.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.18.0 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (0.19.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets>=2.4.0->textattack) (6.0.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->textattack) (2023.3.post1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (3.1.2)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.8,>=1.7.0->textattack) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.4.1)\n",
            "Collecting boto3>=1.20.27 (from flair->textattack)\n",
            "  Downloading boto3-1.33.6-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bpemb>=0.3.2 (from flair->textattack)\n",
            "  Downloading bpemb-0.3.4-py3-none-any.whl (19 kB)\n",
            "Collecting conllu>=4.0 (from flair->textattack)\n",
            "  Downloading conllu-4.5.3-py2.py3-none-any.whl (16 kB)\n",
            "Collecting deprecated>=1.2.13 (from flair->textattack)\n",
            "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting ftfy>=6.1.0 (from flair->textattack)\n",
            "  Downloading ftfy-6.1.3-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: gdown>=4.4.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.6.6)\n",
            "Requirement already satisfied: gensim>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.3.2)\n",
            "Collecting janome>=0.4.2 (from flair->textattack)\n",
            "  Downloading Janome-0.5.0-py2.py3-none-any.whl (19.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langdetect>=1.0.9 (from flair->textattack)\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (4.9.3)\n",
            "Collecting mpld3>=0.3 (from flair->textattack)\n",
            "  Downloading mpld3-0.5.9-py3-none-any.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.2/201.2 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pptree>=3.1 (from flair->textattack)\n",
            "  Downloading pptree-3.1.tar.gz (3.0 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pytorch-revgrad>=0.2.0 (from flair->textattack)\n",
            "  Downloading pytorch_revgrad-0.2.0-py3-none-any.whl (4.6 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.2 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (1.2.2)\n",
            "Collecting segtok>=1.5.11 (from flair->textattack)\n",
            "  Downloading segtok-1.5.11-py3-none-any.whl (24 kB)\n",
            "Collecting sqlitedict>=2.0.0 (from flair->textattack)\n",
            "  Downloading sqlitedict-2.1.0.tar.gz (21 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tabulate>=0.8.10 in /usr/local/lib/python3.10/dist-packages (from flair->textattack) (0.9.0)\n",
            "Collecting transformer-smaller-training-vocab>=0.2.3 (from flair->textattack)\n",
            "  Downloading transformer_smaller_training_vocab-0.3.2-py3-none-any.whl (14 kB)\n",
            "Collecting urllib3<2.0.0,>=1.0.0 (from flair->textattack)\n",
            "  Downloading urllib3-1.26.18-py2.py3-none-any.whl (143 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.8/143.8 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wikipedia-api>=0.5.7 (from flair->textattack)\n",
            "  Downloading Wikipedia_API-0.6.0-py3-none-any.whl (14 kB)\n",
            "Collecting semver<4.0.0,>=3.0.0 (from flair->textattack)\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textattack) (1.3.2)\n",
            "Collecting docopt>=0.6.2 (from num2words->textattack)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting anytree (from OpenHowNet->textattack)\n",
            "  Downloading anytree-2.12.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from OpenHowNet->textattack) (67.7.2)\n",
            "Collecting botocore<1.34.0,>=1.33.6 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading botocore-1.33.6-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.9.0,>=0.8.2 (from boto3>=1.20.27->flair->textattack)\n",
            "  Downloading s3transfer-0.8.2-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentencepiece (from bpemb>=0.3.2->flair->textattack)\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m63.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.13->flair->textattack) (1.14.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.9.3)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets>=2.4.0->textattack) (4.0.3)\n",
            "Requirement already satisfied: wcwidth<0.3.0,>=0.2.12 in /usr/local/lib/python3.10/dist-packages (from ftfy>=6.1.0->flair->textattack) (0.2.12)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (1.16.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from gdown>=4.4.0->flair->textattack) (4.11.2)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.2.0->flair->textattack) (6.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (4.45.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score>=0.3.5->textattack) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (3.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->bert-score>=0.3.5->textattack) (2023.11.17)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.2->flair->textattack) (3.2.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (3.20.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.8,>=1.7.0->textattack) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.8,>=1.7.0->textattack) (1.3.0)\n",
            "Requirement already satisfied: accelerate>=0.20.3 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.30.0->textattack) (0.25.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->gdown>=4.4.0->flair->textattack) (2.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.20.3->transformers>=4.30.0->textattack) (5.9.5)\n",
            "Building wheels for collected packages: pinyin, pycld2, word2number, docopt, langdetect, pptree, sqlitedict\n",
            "  Building wheel for pinyin (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pinyin: filename=pinyin-0.4.0-py3-none-any.whl size=3630476 sha256=9eab5c5939e690186529e402ab2170f4366816ec16fdea0562633d331e243b7f\n",
            "  Stored in directory: /root/.cache/pip/wheels/33/38/af/616fc6f154aa5bae65a1da12b22d79943434269f0468ff9b3f\n",
            "  Building wheel for pycld2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycld2: filename=pycld2-0.41-cp310-cp310-linux_x86_64.whl size=9904067 sha256=0e1953b6b2f680377f57f662bbe9c8aebf475094668ceb7405b1ff24fc759aa0\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/81/31/240c89c845e008a93d98542325270007de595bfd356eb0b06c\n",
            "  Building wheel for word2number (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for word2number: filename=word2number-1.1-py3-none-any.whl size=5568 sha256=56cc17e76316a941c0728120bbf61e059839c6b5c51ed125348db2642716752a\n",
            "  Stored in directory: /root/.cache/pip/wheels/84/ff/26/d3cfbd971e96c5aa3737ecfced81628830d7359b55fbb8ca3b\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=7206794c331df81d7668c4b71e8dd8e2b4626def5160af5beb26c7a08746acfa\n",
            "  Stored in directory: /root/.cache/pip/wheels/fc/ab/d4/5da2067ac95b36618c629a5f93f809425700506f72c9732fac\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=3d0c99ef7122bc10b5111bcb2352407a62b20c2e97523a327795db9a31e360c4\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "  Building wheel for pptree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pptree: filename=pptree-3.1-py3-none-any.whl size=4608 sha256=a7295c87d982a0c28a850457b06c9c343dd2f8384002d504b6decf58dff6767b\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/b6/0e/6f26eb9e6eb53ff2107a7888d72b5a6a597593956113037828\n",
            "  Building wheel for sqlitedict (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sqlitedict: filename=sqlitedict-2.1.0-py3-none-any.whl size=16863 sha256=0c188daa35fd3243ae9c3c7f4b4f72e65cb6a03c061345c09045ad5d8d5c6b41\n",
            "  Stored in directory: /root/.cache/pip/wheels/79/d6/e7/304e0e6cb2221022c26d8161f7c23cd4f259a9e41e8bbcfabd\n",
            "Successfully built pinyin pycld2 word2number docopt langdetect pptree sqlitedict\n",
            "Installing collected packages: word2number, sqlitedict, sentencepiece, pycld2, pptree, pinyin, janome, docopt, urllib3, terminaltables, semver, segtok, num2words, lru-dict, lemminflect, langdetect, jmespath, ftfy, deprecated, conllu, click, anytree, botocore, wikipedia-api, s3transfer, pytorch-revgrad, OpenHowNet, mpld3, language-tool-python, bpemb, boto3, bert-score, transformer-smaller-training-vocab, flair, textattack\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.7\n",
            "    Uninstalling urllib3-2.0.7:\n",
            "      Successfully uninstalled urllib3-2.0.7\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.1.7\n",
            "    Uninstalling click-8.1.7:\n",
            "      Successfully uninstalled click-8.1.7\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires fastapi, which is not installed.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "lida 0.0.10 requires python-multipart, which is not installed.\n",
            "lida 0.0.10 requires uvicorn, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed OpenHowNet-2.0 anytree-2.12.1 bert-score-0.3.13 boto3-1.33.6 botocore-1.33.6 bpemb-0.3.4 click-8.0.4 conllu-4.5.3 deprecated-1.2.14 docopt-0.6.2 flair-0.13.0 ftfy-6.1.3 janome-0.5.0 jmespath-1.0.1 langdetect-1.0.9 language-tool-python-2.7.1 lemminflect-0.2.3 lru-dict-1.3.0 mpld3-0.5.9 num2words-0.5.13 pinyin-0.4.0 pptree-3.1 pycld2-0.41 pytorch-revgrad-0.2.0 s3transfer-0.8.2 segtok-1.5.11 semver-3.0.2 sentencepiece-0.1.99 sqlitedict-2.1.0 terminaltables-3.1.10 textattack-0.3.9 transformer-smaller-training-vocab-0.3.2 urllib3-1.26.18 wikipedia-api-0.6.0 word2number-1.1\n"
          ]
        }
      ],
      "source": [
        "!pip install textattack"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "f-qu4T169eSW",
        "outputId": "ee06f0d0-6785-4f01-e866-6d2a3ee14225"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>premise</th>\n",
              "      <th>hypothesis</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>A person is training his horse for a competition.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>A person is at a diner, ordering an omelette.</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>A person on a horse jumps over a broken down a...</td>\n",
              "      <td>A person is outdoors, on a horse.</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>They are smiling at their parents</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Children smiling and waving at camera</td>\n",
              "      <td>There are children present</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550147</th>\n",
              "      <td>Four dirty and barefooted children.</td>\n",
              "      <td>four kids won awards for 'cleanest feet'</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550148</th>\n",
              "      <td>Four dirty and barefooted children.</td>\n",
              "      <td>four homeless children had their shoes stolen,...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550149</th>\n",
              "      <td>A man is surfing in a bodysuit in beautiful bl...</td>\n",
              "      <td>A man in a bodysuit is competing in a surfing ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550150</th>\n",
              "      <td>A man is surfing in a bodysuit in beautiful bl...</td>\n",
              "      <td>A man in a business suit is heading to a board...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>550151</th>\n",
              "      <td>A man is surfing in a bodysuit in beautiful bl...</td>\n",
              "      <td>On the beautiful blue water there is a man in ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>550152 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                  premise  \\\n",
              "0       A person on a horse jumps over a broken down a...   \n",
              "1       A person on a horse jumps over a broken down a...   \n",
              "2       A person on a horse jumps over a broken down a...   \n",
              "3                   Children smiling and waving at camera   \n",
              "4                   Children smiling and waving at camera   \n",
              "...                                                   ...   \n",
              "550147                Four dirty and barefooted children.   \n",
              "550148                Four dirty and barefooted children.   \n",
              "550149  A man is surfing in a bodysuit in beautiful bl...   \n",
              "550150  A man is surfing in a bodysuit in beautiful bl...   \n",
              "550151  A man is surfing in a bodysuit in beautiful bl...   \n",
              "\n",
              "                                               hypothesis  label  \n",
              "0       A person is training his horse for a competition.      1  \n",
              "1           A person is at a diner, ordering an omelette.      2  \n",
              "2                       A person is outdoors, on a horse.      0  \n",
              "3                       They are smiling at their parents      1  \n",
              "4                              There are children present      0  \n",
              "...                                                   ...    ...  \n",
              "550147           four kids won awards for 'cleanest feet'      2  \n",
              "550148  four homeless children had their shoes stolen,...      1  \n",
              "550149  A man in a bodysuit is competing in a surfing ...      1  \n",
              "550150  A man in a business suit is heading to a board...      2  \n",
              "550151  On the beautiful blue water there is a man in ...      0  \n",
              "\n",
              "[550152 rows x 3 columns]"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = 'snlitraindata.csv'\n",
        "df_ad = pd.read_csv(file_path)\n",
        "df_ad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_k_bTwUQfNgI"
      },
      "outputs": [],
      "source": [
        "# ! pip install --upgrade pip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFanndQxLqFQ",
        "outputId": "f700f0f7-e87c-4dde-a084-2a25d37d235e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.18)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84_1r9RhL7KZ",
        "outputId": "e7a7564a-a230-4e4a-950d-71302b9dd8bc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "textattack: Updating TextAttack package dependencies.\n",
            "textattack: Downloading NLTK required packages.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package omw to /root/nltk_data...\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import textattack\n",
        "\n",
        "def apply_attack_to_premise(dataset, attack):\n",
        "    transformed_premise_data = []\n",
        "    for original_premise, original_hypothesis, label in dataset:\n",
        "        attack_results = attack.attack(original_premise, label)\n",
        "\n",
        "        if hasattr(attack_results, '__iter__'):\n",
        "            for result in attack_results:\n",
        "                if isinstance(result, textattack.attack_results.SuccessfulAttackResult):\n",
        "                    transformed_premise_data.append((original_premise, result.perturbed_text(), original_hypothesis, label))\n",
        "        else:\n",
        "            if isinstance(attack_results, textattack.attack_results.SuccessfulAttackResult):\n",
        "                transformed_premise_data.append((original_premise, attack_results.perturbed_text(), original_hypothesis, label))\n",
        "    return transformed_premise_data\n",
        "\n",
        "def apply_attack_to_hypothesis(dataset, attack):\n",
        "    transformed_hypothesis_data = []\n",
        "    for original_premise, original_hypothesis, label in dataset:\n",
        "        attack_results = attack.attack(original_hypothesis, label)\n",
        "\n",
        "        if hasattr(attack_results, '__iter__'):\n",
        "            for result in attack_results:\n",
        "                if isinstance(result, textattack.attack_results.SuccessfulAttackResult):\n",
        "                    transformed_hypothesis_data.append((original_premise, original_hypothesis, result.perturbed_text(), label))\n",
        "        else:\n",
        "            if isinstance(attack_results, textattack.attack_results.SuccessfulAttackResult):\n",
        "                transformed_hypothesis_data.append((original_premise, original_hypothesis, attack_results.perturbed_text(), label))\n",
        "    return transformed_hypothesis_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YwYz3hZV9v8D"
      },
      "outputs": [],
      "source": [
        "from textattack.constraints.pre_transformation import RepeatModification, StopwordModification\n",
        "from textattack.constraints.semantics.sentence_encoders import UniversalSentenceEncoder\n",
        "from textattack.constraints.grammaticality import PartOfSpeech\n",
        "from textattack.transformations import WordSwapEmbedding\n",
        "from textattack.search_methods import GreedySearch\n",
        "from textattack.goal_functions import UntargetedClassification\n",
        "from textattack import Attack\n",
        "\n",
        "class CustomRecipe(Attack):\n",
        "    def __init__(self, model):\n",
        "        transformation = WordSwapEmbedding(max_candidates=10)\n",
        "        constraints = [\n",
        "            RepeatModification(),\n",
        "            StopwordModification(),\n",
        "            UniversalSentenceEncoder(threshold=0.8),\n",
        "            PartOfSpeech()\n",
        "        ]\n",
        "\n",
        "        search_method = GreedySearch()\n",
        "        goal_function = UntargetedClassification(model)\n",
        "        super().__init__(goal_function, constraints, transformation, search_method)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sIGxiQKNdLj",
        "outputId": "14995a89-5905-475a-fc09-f463274dbff8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/menghao_yang/miniconda3/envs/torch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "2023-12-03 02:29:42.150896: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        },
        {
          "ename": "HFValidationError",
          "evalue": "Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/NLP_FP/nlp_fp/trained_model/checkpoint/checkpoint-206000'. Use `repo_type` argument if needed.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHFValidationError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ElectraForSequenceClassification\n\u001b[1;32m      6\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/NLP_FP/nlp_fp/trained_model/checkpoint/checkpoint-206000\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 7\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mElectraForSequenceClassification\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path)\n\u001b[1;32m     11\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/modeling_utils.py:2600\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   2597\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m commit_hash \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   2598\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(config, PretrainedConfig):\n\u001b[1;32m   2599\u001b[0m         \u001b[38;5;66;03m# We make a call to the config file first (which may be absent) to get the commit hash as soon as possible\u001b[39;00m\n\u001b[0;32m-> 2600\u001b[0m         resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2601\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2602\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2603\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2604\u001b[0m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2605\u001b[0m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2606\u001b[0m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2607\u001b[0m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2608\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2609\u001b[0m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2610\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2611\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2612\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   2613\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2614\u001b[0m         commit_hash \u001b[38;5;241m=\u001b[39m extract_commit_hash(resolved_config_file, commit_hash)\n\u001b[1;32m   2615\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/transformers/utils/hub.py:430\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    427\u001b[0m user_agent \u001b[38;5;241m=\u001b[39m http_user_agent(user_agent)\n\u001b[1;32m    428\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    429\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    445\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    446\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to request access at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    447\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and pass a token having permission to this repo either \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    448\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby logging in with `huggingface-cli login` or by passing `token=<your_token>`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    449\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:110\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m arg_name, arg_value \u001b[38;5;129;01min\u001b[39;00m chain(\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mzip\u001b[39m(signature\u001b[38;5;241m.\u001b[39mparameters, args),  \u001b[38;5;66;03m# Args values\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mitems(),  \u001b[38;5;66;03m# Kwargs values\u001b[39;00m\n\u001b[1;32m    108\u001b[0m ):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m arg_name \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrepo_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfrom_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m--> 110\u001b[0m         \u001b[43mvalidate_repo_id\u001b[49m\u001b[43m(\u001b[49m\u001b[43marg_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m arg_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m arg_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         has_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/torch/lib/python3.11/site-packages/huggingface_hub/utils/_validators.py:158\u001b[0m, in \u001b[0;36mvalidate_repo_id\u001b[0;34m(repo_id)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be a string, not \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(repo_id)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m repo_id\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 158\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must be in the form \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnamespace/repo_name\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Use `repo_type` argument if needed.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    161\u001b[0m     )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m REPO_ID_REGEX\u001b[38;5;241m.\u001b[39mmatch(repo_id):\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HFValidationError(\n\u001b[1;32m    165\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRepo id must use alphanumeric chars or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m--\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m are\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m forbidden, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m cannot start or end the name, max length is 96:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    167\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrepo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    168\u001b[0m     )\n",
            "\u001b[0;31mHFValidationError\u001b[0m: Repo id must be in the form 'repo_name' or 'namespace/repo_name': '/content/NLP_FP/nlp_fp/trained_model/checkpoint/checkpoint-206000'. Use `repo_type` argument if needed."
          ]
        }
      ],
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "from textattack.models.wrappers import HuggingFaceModelWrapper\n",
        "from textattack.datasets import Dataset\n",
        "from transformers import ElectraForSequenceClassification\n",
        "\n",
        "model_path = '/home/menghao_yang/workspace/fp-dataset-artifacts/trained_model/checkpoint-pretrain'\n",
        "model = ElectraForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "\n",
        "model.eval()\n",
        "\n",
        "# Wrap the model for TextAttack\n",
        "model_wrapper = HuggingFaceModelWrapper(model, tokenizer)\n",
        "\n",
        "# Initialize your TextAttack custom recipe with the model wrapper\n",
        "attack = CustomRecipe(model_wrapper)\n",
        "\n",
        "# Define a class for your custom dataset\n",
        "class CustomTextAttackDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "num_examples = 200\n",
        "\n",
        "combined_data = [(row['premise'], row['hypothesis'], row['label']) for _, row in df_ad.iterrows()]\n",
        "\n",
        "for ite in range(100):\n",
        "    limited_combined_data = combined_data[ite*num_examples:(ite+1)*num_examples]\n",
        "    custom_dataset = CustomTextAttackDataset(limited_combined_data)\n",
        "\n",
        "    # Apply the attack to the premise\n",
        "    transformed_premise_data = apply_attack_to_premise(custom_dataset, attack)\n",
        "    import json\n",
        "\n",
        "    # Structure transformed_premise_data for JSON\n",
        "    structured_premise_data = [\n",
        "        {\n",
        "            'original_premise': orig_premise,\n",
        "            'adversarial_premise': adv_premise,\n",
        "            'original_hypothesis': orig_hypothesis,\n",
        "            'label': label\n",
        "        }\n",
        "        for orig_premise, adv_premise, orig_hypothesis, label in transformed_premise_data\n",
        "    ]\n",
        "\n",
        "    # Save structured_premise_data to a JSON file\n",
        "    with open('transformed_premise_data_4.json', 'w') as outfile:\n",
        "        json.dump(structured_premise_data, outfile, indent=4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CehQxdmkVNbB"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "transformed_hypothesis_data = apply_attack_to_hypothesis(custom_dataset, attack)\n",
        "structured_hypothesis_data = [\n",
        "    {\n",
        "        'original_premise': orig_premise,\n",
        "        'original_hypothesis': orig_hypothesis,\n",
        "        'adversarial_hypothesis': adv_hypothesis,\n",
        "        'label': label\n",
        "    }\n",
        "    for orig_premise, orig_hypothesis, adv_hypothesis, label in transformed_hypothesis_data\n",
        "]\n",
        "\n",
        "with open('transformed_hypothesis_data_4.json', 'w') as outfile:\n",
        "    json.dump(structured_hypothesis_data, outfile, indent=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zrmriQI1uIfg"
      },
      "outputs": [],
      "source": [
        "!git add .\n",
        "!git config --global user.email \"joliefang@utexas.edu\"\n",
        "!git config --global user.name \"JoFangUTA\"\n",
        "!git commit -m \"message\"\n",
        "!git push origin main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UUk29BoZmEF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "file_path_og = '/content/NLP_FP/nlp_fp/transfer/snli_dataset/snli_test.jsonl'\n",
        "\n",
        "data = []\n",
        "\n",
        "with open(file_path_og, 'r') as file:\n",
        "    for line in file:\n",
        "        json_obj = json.loads(line)\n",
        "        data.append(json_obj)\n",
        "\n",
        "df_og = pd.DataFrame(data)\n",
        "\n",
        "df_og.head(5)\n",
        "df_og.to_csv('/content/NLP_FP/nlp_fp/transfer/clean_dataset/snlitestdata.csv',index = False)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "c3SQaiQBtKK5"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
